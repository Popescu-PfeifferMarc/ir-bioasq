{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8nRAB_ePUKGg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import timeit\n",
        "import random\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "from IPython.display import display, HTML"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the data"
      ],
      "metadata": {
        "id": "2Fe6xjpzWFmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('./data/', exist_ok=True)"
      ],
      "metadata": {
        "id": "aVO71o1JWEoE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "URL = 'https://raw.githubusercontent.com/Popescu-PfeifferMarc/ir-bioasq/refs/heads/master/dataset/training12b_new.json?token=GHSAT0AAAAAAC2O7C4H2NXRKDBW252PUUMKZ2CHVLA'\n",
        "DATA_PATH = './data/training12B.json'"
      ],
      "metadata": {
        "id": "zN60glJ6YNGw"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = requests.get(URL)\n",
        "try:\n",
        "  data_dict = res.json()\n",
        "except json.JSONDecodeError as e:\n",
        "  print(\"Error parsing JSON: \", e)\n",
        "  raise\n",
        "\n",
        "with open(DATA_PATH, 'wb') as f:\n",
        "  f.write(res.content)\n",
        "\n",
        "with open(DATA_PATH, 'rb') as f:\n",
        "  data_dict = json.load(f)"
      ],
      "metadata": {
        "id": "gkSXY8XCW5Ad"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading the data"
      ],
      "metadata": {
        "id": "Mozgo1vbXs8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dict.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddhJ8LK8YkBH",
        "outputId": "976cf15c-b762-43af-d41c-c0c7c73f42f8"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['questions'])"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "questions = []\n",
        "answers = []\n",
        "context = []\n",
        "\n",
        "for question in data_dict['questions']:\n",
        "    questions.append(question['body'])\n",
        "\n",
        "    if question['snippets']:\n",
        "        longest_entry = max(question['snippets'], key=lambda x: len(x[\"text\"]))\n",
        "        context.append(longest_entry['text'])\n",
        "\n",
        "        answers.append(dict(\n",
        "            text=question.get('ideal_answer', [None])[0],\n",
        "            start_idx=longest_entry['offsetInBeginSection'],\n",
        "            end_idx=longest_entry['offsetInEndSection']))"
      ],
      "metadata": {
        "id": "kXLzqngmXzig"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# questions = []\n",
        "# answers = []\n",
        "# context = []\n",
        "\n",
        "# for q in data_dict['questions']:\n",
        "#   questions.append(q['body'])\n",
        "#   context.append(q['snippets'][0]['text'])\n",
        "\n",
        "#   answers.append(dict(text=q['ideal_answer'][0],\n",
        "#                       start_idx=q['snippets'][0]['offsetInBeginSection'],\n",
        "#                       end_idx= q['snippets'][0]['offsetInEndSection']))"
      ],
      "metadata": {
        "id": "jg0-fFHvYlwu"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(questions), len(answers), len(context)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18Aowx44rylC",
        "outputId": "28a45f70-a36a-41dc-87db-60e1466e82ad"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5049, 5049, 5049)"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_questions, val_questions, train_answers, val_answers, train_context, val_context = train_test_split(questions,\n",
        "                                                                                                          answers,\n",
        "                                                                                                          context,\n",
        "                                                                                                          test_size=0.2,\n",
        "                                                                                                          random_state=42)"
      ],
      "metadata": {
        "id": "wUsdlMXKtflQ"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_questions, val_questions, test_answers, val_answers, test_context, val_context = train_test_split(val_questions,\n",
        "                                                                                                       val_answers,\n",
        "                                                                                                       val_context,\n",
        "                                                                                                       test_size=0.5,\n",
        "                                                                                                       random_state=42)"
      ],
      "metadata": {
        "id": "Tjkfd2edt3ig"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_questions), len(val_questions), len(test_questions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CC3wp0ftu5a",
        "outputId": "f6f529a9-e925-4f08-db9b-88a4f565d522"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4039, 505, 505)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_answers), len(val_answers), len(test_answers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6T5y8MVOuIGA",
        "outputId": "4fded5e8-ae6e-4992-b662-7f847f48e18b"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4039, 505, 505)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_context), len(val_context), len(test_context)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NL8SzxruKuG",
        "outputId": "ec04b5de-b43d-47df-b2da-2d6768a2a928"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4039, 505, 505)"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
        "\n",
        "MODEL_NAME = \"deepset/roberta-base-squad2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
      ],
      "metadata": {
        "id": "JnVEwD3lu815"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings = tokenizer(train_context, train_questions, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_context, val_questions, truncation=True, padding=True)\n",
        "test_encodings = tokenizer(test_context, test_questions, truncation=True, padding=True)"
      ],
      "metadata": {
        "id": "RkkNx_cxwafL"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "source": [
        "def add_token_positions(encodings, answers):\n",
        "  # initialize lists to contain the token indices of answer start/end\n",
        "  start_positions = []\n",
        "  end_positions = []\n",
        "  for i in range(len(answers)):\n",
        "    # append start/end token position using char_to_token method\n",
        "    # Check if start_idx is non-negative before calling char_to_token\n",
        "    start_idx = answers[i]['start_idx']\n",
        "    if start_idx >= 0:\n",
        "      start_positions.append(encodings.char_to_token(i, start_idx))\n",
        "    else:\n",
        "      # Handle negative start_idx, e.g., set to 0 or skip\n",
        "      start_positions.append(0)  # or None, depending on your logic\n",
        "\n",
        "    # Check if end_idx is non-negative before calling char_to_token\n",
        "    end_idx = answers[i]['end_idx']\n",
        "    if end_idx >= 0:\n",
        "      end_positions.append(encodings.char_to_token(i, end_idx))\n",
        "    else:\n",
        "      # Handle negative end_idx, e.g., set to 0 or skip\n",
        "      end_positions.append(0)  # or None, depending on your logic\n",
        "\n",
        "    # if start position is None, the answer passage has been truncated\n",
        "    if start_positions[-1] is None:\n",
        "      start_positions[-1] = tokenizer.model_max_length\n",
        "    # end position cannot be found, char_to_token found space, so shift one token forward\n",
        "    go_back = 1\n",
        "    while end_positions[-1] is None:\n",
        "      end_pos_idx = answers[i]['end_idx'] - go_back\n",
        "      # Check if end_pos_idx is non-negative before calling char_to_token\n",
        "      if end_pos_idx >= 0:\n",
        "        end_positions[-1] = encodings.char_to_token(i, end_pos_idx)\n",
        "      else:\n",
        "        # Handle negative end_pos_idx, e.g., set to 0 or break\n",
        "        end_positions[-1] = 0  # or break the loop\n",
        "        break\n",
        "      go_back += 1\n",
        "  # update our encodings object with the new token-based start/end positions\n",
        "  encodings.update({'start_positions': start_positions, 'end_positions': end_positions})"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "WR6YM5yvxpUo"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "add_token_positions(train_encodings, train_answers)\n",
        "add_token_positions(val_encodings, val_answers)\n",
        "add_token_positions(test_encodings, test_answers)"
      ],
      "metadata": {
        "id": "YKenZ96YxGAA"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BioASQDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)\n",
        "\n",
        "train_dataset = BioASQDataset(train_encodings)\n",
        "val_dataset = BioASQDataset(val_encodings)\n",
        "test_dataset = BioASQDataset(test_encodings)"
      ],
      "metadata": {
        "id": "xBxeBO0wxJNz"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSR-OYmDHWDR",
        "outputId": "2e24c031-b992-415f-e4b5-da466d50ea0d"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -rf /content/drive/MyDrive/models/deepset"
      ],
      "metadata": {
        "id": "AJWnTTBmZLad"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LEARNING_RATE = 5e-5\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 3\n",
        "MODEL_SAVE_PATH = f\"/content/drive/MyDrive/models/{MODEL_NAME}-lr{LEARNING_RATE}-epoch{EPOCHS}-v2/\""
      ],
      "metadata": {
        "id": "UMZEh7CWyoJ3"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformers.utils.logging.set_verbosity_error()\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "oYaHTErwoMlb"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME).to(device)\n",
        "\n",
        "print(model.num_parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYwMp6z22MI2",
        "outputId": "dbd8822f-2957-4da0-8f36-2682f3484fa6"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "124056578\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                               batch_size=BATCH_SIZE,\n",
        "                                               shuffle=True)\n",
        "\n",
        "val_dataloader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
        "                                             batch_size=BATCH_SIZE,\n",
        "                                             shuffle=False)\n",
        "\n",
        "test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                              batch_size=BATCH_SIZE,\n",
        "                                              shuffle=False)"
      ],
      "metadata": {
        "id": "siBhRD0221e5"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "start = timeit.default_timer()\n",
        "for epoch in range(EPOCHS):\n",
        "  model.train()\n",
        "  train_running_loss = 0\n",
        "  for idx, sample in enumerate(tqdm(train_dataloader, leave=True)):\n",
        "    input_ids = sample['input_ids'].to(device)\n",
        "    attention_mask = sample['attention_mask'].to(device)\n",
        "    start_positions = sample['start_positions'].to(device)\n",
        "    end_positions = sample['end_positions'].to(device)\n",
        "    outputs = model(input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    start_positions=start_positions,\n",
        "                    end_positions=end_positions)\n",
        "\n",
        "    loss = outputs.loss\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_running_loss += loss.item()\n",
        "\n",
        "  train_loss = train_running_loss / (idx + 1)\n",
        "\n",
        "  model.eval()\n",
        "  val_running_loss = 0\n",
        "  with torch.inference_mode():\n",
        "    for idx, sample in enumerate(tqdm(val_dataloader)):\n",
        "      input_ids = sample['input_ids'].to(device)\n",
        "      attention_mask = sample['attention_mask'].to(device)\n",
        "      start_positions = sample['start_positions'].to(device)\n",
        "      end_positions = sample['end_positions'].to(device)\n",
        "      outputs = model(input_ids=input_ids,\n",
        "                      attention_mask=attention_mask,\n",
        "                      start_positions=start_positions,\n",
        "                      end_positions=end_positions)\n",
        "\n",
        "      val_running_loss += outputs.loss.item()\n",
        "    val_loss = val_running_loss / (idx + 1)\n",
        "\n",
        "  print(\"-\"*30)\n",
        "  print(f\"EPOCH: {epoch+1:02d} | Train Loss: {train_loss:.4f}\")\n",
        "  print(f\"EPOCH: {epoch+1:02d} | Valid Loss: {val_loss:.4f}\")\n",
        "  print(\"-\"*30)\n",
        "  stop = timeit.default_timer()\n",
        "  print(f\"Training Time: {stop-start:.2f}s\")\n",
        "\n",
        "  model.save_pretrained(MODEL_SAVE_PATH)\n",
        "  tokenizer.save_pretrained(MODEL_SAVE_PATH)\n",
        "\n",
        "  torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "LyWWCarg3i3R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3660b6b-f78c-457c-e55f-ee1cdf2102c8"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 253/253 [06:34<00:00,  1.56s/it]\n",
            "100%|██████████| 32/32 [00:13<00:00,  2.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------\n",
            "EPOCH: 01 | Train Loss: 1.4969\n",
            "EPOCH: 01 | Valid Loss: 1.1446\n",
            "------------------------------\n",
            "Training Time: 408.44s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 253/253 [06:33<00:00,  1.56s/it]\n",
            "100%|██████████| 32/32 [00:13<00:00,  2.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------\n",
            "EPOCH: 02 | Train Loss: 1.2079\n",
            "EPOCH: 02 | Valid Loss: 1.1976\n",
            "------------------------------\n",
            "Training Time: 818.79s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 253/253 [06:33<00:00,  1.56s/it]\n",
            "100%|██████████| 32/32 [00:13<00:00,  2.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------\n",
            "EPOCH: 03 | Train Loss: 1.0674\n",
            "EPOCH: 03 | Valid Loss: 1.1376\n",
            "------------------------------\n",
            "Training Time: 1229.30s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = []\n",
        "true = []\n",
        "running_accuracy = []\n",
        "\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "  for idx, sample in enumerate(tqdm(test_dataloader, leave=True)):\n",
        "    input_ids = sample['input_ids'].to(device)\n",
        "    attention_mask = sample['attention_mask'].to(device)\n",
        "    start_positions = sample['start_positions']\n",
        "    end_positions = sample['end_positions']\n",
        "\n",
        "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    start_pred = torch.argmax(outputs['start_logits'], dim=1).cpu().detach()\n",
        "    end_pred = torch.argmax(outputs['end_logits'], dim=1).cpu().detach()\n",
        "\n",
        "    preds.extend([[int(i), int(j)] for i, j in zip(start_pred, end_pred)])\n",
        "    true.extend([[int(i), int(j)] for i, j in zip(start_positions, end_positions)])\n",
        "\n",
        "    running_accuracy.append(((start_pred == start_positions).sum()/len(start_positions)).item())\n",
        "    running_accuracy.append(((end_pred == end_positions).sum()/len(end_positions)).item())\n",
        "\n",
        "preds = [item for sublist in preds for item in sublist]\n",
        "true = [item for sublist in true for item in sublist]\n",
        "\n",
        "accuracy = sum(running_accuracy)/len(running_accuracy) # average accuracy\n",
        "f1_value = f1_score(true, preds, average=\"macro\")\n",
        "print(f\"\\nAccuracy: {accuracy*100:.2f}% | F1 Score: {f1_value*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5k3i6sI4m8A",
        "outputId": "05b7cfc5-6eae-4fba-cde4-2af839122e96"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:10<00:00,  3.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 69.01% | F1 Score: 79.90%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def model_inference(question, context, model_path=MODEL_SAVE_PATH):\n",
        "  model = AutoModelForQuestionAnswering.from_pretrained(model_path)\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "  start = timeit.default_timer()\n",
        "  qa_model = pipeline(task=\"question-answering\", model=model, tokenizer=tokenizer)\n",
        "  stop = timeit.default_timer()\n",
        "  print(f\"Inference Time: {stop-start:.2f}s\")\n",
        "  return qa_model(question=question, context=context)"
      ],
      "metadata": {
        "id": "rDKEP4aQ7pb-"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_result(context, result):\n",
        "  before_text = context[:result['start']]\n",
        "  colored_text = context[result['start']:result['end']+1]\n",
        "  after_text = context[result['end']+1:]\n",
        "  # print(f\"Answer: {result['answer']}\")\n",
        "  display(HTML(f\"\"\"<p style='font-size: 16px; width: 50%;'>{before_text}\n",
        "    <span style='background-color: #33447f; color: white; width: {len(result[\"answer\"])}em;'>{colored_text}</span>\n",
        "    {after_text}</p>\"\"\"))"
      ],
      "metadata": {
        "id": "pd7lU7e-gVMy"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_idx = random.randint(0, len(test_questions)-1)\n",
        "question = test_questions[random_idx]\n",
        "context = test_context[random_idx]\n",
        "print(question)\n",
        "result = model_inference(question, context)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RN9JEcVHQsoF",
        "outputId": "5f87c945-e0c1-4316-aea7-cd6ebf4fd6c2"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is Jackhammer esophagus?\n",
            "Inference Time: 0.00s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'score': 0.009454484097659588,\n",
              " 'start': 122,\n",
              " 'end': 134,\n",
              " 'answer': 'peristalsis.'}"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display_result(context, result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "I2fL_676PbnW",
        "outputId": "b1e46f55-3fb7-4cf0-ca75-7dae0acfa56f"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p style='font-size: 16px; width: 50%;'>Jackhammer esophagus (JE) is a recently recognized esophageal motility disorder that is characterized by hypercontractile \n",
              "    <span style='background-color: #33447f; color: white; width: 12em;'>peristalsis.</span>\n",
              "    </p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O_BMtXTWqnVR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from extract_xml import extract_xml\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "dataset_folder = './dataset/pubmed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch processing function to load documents\n",
    "def load_documents_in_batches(folder_path, batch_size):\n",
    "    batch = []\n",
    "    total_files = len(os.listdir(folder_path))\n",
    "    start_time = time.time()\n",
    "    for idx, filename in enumerate(os.listdir(folder_path)):\n",
    "        content = extract_xml(os.path.join(folder_path, filename))\n",
    "        if content[\"title\"] is None or content[\"body\"] is None:\n",
    "            continue\n",
    "        batch.append(content[\"title\"] + \" \" + content[\"body\"])\n",
    "        \n",
    "        # Yield a batch of documents when batch_size is reached\n",
    "        if len(batch) == batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "            elapsed_time = time.time() - start_time\n",
    "            time_per_file = elapsed_time / (idx + 1)\n",
    "            time_left = time_per_file * (total_files - (idx + 1))\n",
    "            print(f\"Loaded {idx + 1} / {total_files} documents, Elapsed time: {elapsed_time:.2f}s, Time left: {time_left:.2f}s\")\n",
    "    if batch:\n",
    "        yield batch  # Return the remaining documents in the last batch\n",
    "        print(f\"Loaded {total_files} / {total_files} documents in {time.time() - start_time:.2f}s\")\n",
    "\n",
    "# Function to process documents in batches\n",
    "def process_documents_in_batches(folder_path, batch_size=100):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=10_000)  # Use max_features to limit vocabulary size\n",
    "    partial_results = []\n",
    "    batch_count = 0\n",
    "    \n",
    "    for batch in load_documents_in_batches(folder_path, batch_size):\n",
    "        # Incrementally fit and transform the batches\n",
    "        tfidf_matrix = vectorizer.fit_transform(batch)\n",
    "        partial_results.append(tfidf_matrix.toarray())\n",
    "        batch_count += 1\n",
    "        print(f\"Processed {batch_count * batch_size} documents\")\n",
    "    \n",
    "    # Concatenate results from all batches\n",
    "    tfidf_full_matrix = np.vstack(partial_results)\n",
    "    return tfidf_full_matrix, vectorizer.get_feature_names_out()\n",
    "\n",
    "# Process in batches (adjust batch_size based on available memory)\n",
    "tfidf_matrix, feature_names = process_documents_in_batches(dataset_folder, batch_size=100)\n",
    "\n",
    "# Convert to DataFrame and save to CSV\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix, columns=feature_names)\n",
    "df_tfidf.to_csv('tfidf_results_large.csv')\n",
    "\n",
    "print(\"TF-IDF results saved to 'tfidf_results_large.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

\documentclass{article}
\usepackage{graphicx} % Required for inserting images

\bibliographystyle{plain}

\title{Project Report\\
BioASQ Task-13b - Biomedical Semantic QA\\
Information Retrieval and Web Search\\
\vspace{0.5cm}
University of Mannheim}

\author{Onur Can Memiş (2040827), Abdullah Al Momin (1869371),\\Marc Popescu-Pfeiffer (1638093), Enes Saban Tanrikulu(2055430),\\Basar Temur (1909058)}

\date{\today}

\begin{document}

\maketitle
\pagebreak
\tableofcontents

\pagebreak

\section{Introduction}
Biomedical question answering (QA) is a complex task requiring accurate information retrieval and precise answer generation from large, domain-specific datasets. In this project, we participated in BioASQ Task-13b, leveraging the PubMed dataset to explore and evaluate independent models for document retrieval and QA. For retrieval, we used Vector Space Models (TF-IDF with raw counts and logarithmic smoothing) and the probabilistic BM25 model, ranking documents based on relevance. For QA, we employed Llama 3.1, a large language model, and \texttt{deepset/roberta-base-squad2}, a fine-tuned span-based QA model. Each model was developed and assessed independently, providing valuable insights into their performance in biomedical QA.


This report outlines our approach's methodology, implementation, and evaluation for BioASQ Task-13b, providing insights into its performance and limitations. The following sections detail our system design, dataset preprocessing, model training, and results.

\section{Dataset Description}
For the BioASQ Task-13b, the PubMed dataset was utilized as the primary source for question-answering. It provides rich, structured metadata such as titles, questions, answers, contexts, relevant documents, publication dates, and answer begin and end indices. These attributes enable targeted retrieval and semantic understanding of biomedical queries.


For our project, we specifically focused on curated subsets of PubMed data relevant to the BioASQ challenge. This involved pre-processing abstracts and associated metadata to align with the question-answering requirements of Task-13b. The dataset’s breadth and quality presented both opportunities and challenges in system development, necessitating efficient retrieval strategies and sophisticated NLP techniques to handle the complexity of biomedical language.


The following sections will detail our preprocessing steps, the challenges encountered with the dataset, and how we addressed them to develop an effective question-answering system.


\section{Methodology}
Our approach for BioASQ Task-13b aimed to evaluate the effectiveness of different methodologies in biomedical question answering. Each component was implemented and assessed independently, focusing on specific tasks such as relevant document retrieval and answer generation. The following models and techniques were used:

\subsection{Relevant Document Retrieval}
To identify documents from the PubMed dataset that were most relevant to a query, we implemented and evaluated three independent retrieval models:

\begin{enumerate}
    \item \textbf{Vector Space Models (VSM):}
        \begin{itemize}
            \item \textbf{TF-IDF Model with Raw Counts:}\\
            This model ranked documents by computing Term Frequency-Inverse Document Frequency (TF-IDF) scores using raw term frequencies. It served as a baseline for retrieval performance.
            \item \textbf{TF-IDF Model with Logarithmic Smoothing:}\\
            Logarithmic scaling was applied to term frequencies in this variation, reducing the influence of excessively frequent terms and enhancing the balance between term importance and query relevance.
        \end{itemize}
    \item \textbf{Probabilistic Model:}
        \begin{itemize}
            \item \textbf{BM25:}\\
            The BM25 model was implemented to rank documents probabilistically, accounting for term frequency, inverse document frequency, and document length. Its ability to adapt to varying document lengths in the PubMed dataset made it particularly effective for biomedical literature.
        \end{itemize}
\end{enumerate}

\subsection{Question Answering (QA)}
For generating precise answers to biomedical questions, we evaluated two advanced natural language processing models independently:

\begin{enumerate}
    \item \textbf{Large Language Model (LLM):}
    \begin{itemize}
        \item \textbf{Llama 3.1 (\texttt{llama3.1:8b}):}\\
        This 8-billion-parameter model was utilized to understand and generate comprehensive answers directly from biomedical texts. The Llama 3.1 model was evaluated for its ability to generate contextually relevant answers in response to domain-specific queries.
    \end{itemize}
    \item \textbf{Fine-Tuned QA Model:}
    \begin{itemize}
        \item \textbf{\texttt{deepset/roberta-base-squad2}:}\\
        A pre-trained RoBERTa model fine-tuned on the SQuAD2.0 dataset was employed to extract span-based answers from relevant documents. It focused on identifying concise and precise responses within text segments.
    \end{itemize}
\end{enumerate}

\subsection{Approach}
Each model was run independently on the provided questions from BioASQ Task-13b, and their respective outputs were collected and analyzed. The results from retrieval models were evaluated based on their ability to fetch relevant documents, while the QA models were assessed for the quality and accuracy of their answers.\\

This modular approach allowed us to analyze the individual strengths and weaknesses of each model. The findings will provide insights into how these models can be further refined or integrated in future iterations for improved performance in biomedical question-answering tasks.

\section{Implementation}
The implementation of our system for BioASQ Task-13b focused on evaluating individual models for document retrieval and question answering. Each component was implemented and assessed independently without connecting the outputs of the retrieval models to the QA models. Below, we outline the implementation details.

\subsection{Relevant Document Retrieval}
\begin{enumerate}
    \item \textbf{Data Preprocessing}
        \begin{itemize}
            \item Titles, abstracts, and metadata from the PubMed dataset were extracted for document indexing.
            \item Standard preprocessing steps such as lowercasing, punctuation removal, tokenization, and stopword filtering were applied.
            \item Documents were vectorized for computational efficiency.
        \end{itemize}
    \item \textbf{Implementation of Retrieval Models}
    \begin{itemize}
        \item \textbf{TF-IDF with Raw Counts:}
        \begin{itemize}
            \item Implemented using scikit-learn’s \texttt{TfidfVectorizer}.
            \item Queries and documents were vectorized, and cosine similarity was computed to rank documents by relevance.
        \end{itemize}
        \item \textbf{TF-IDF with Logarithmic Smoothing:}
        \begin{itemize}
            \item Logarithmic scaling was applied to term frequencies during TF-IDF computation.
            \item Document-query relevance was evaluated using cosine similarity, emphasizing balanced term importance.
        \end{itemize}
        \item \textbf{BM25:}
        \begin{itemize}
            \item The \texttt{rank\_bm25} library was used for probabilistic document ranking.
            \item Model parameters $k_1$ and $b$ were tuned to optimize retrieval for biomedical text.
        \end{itemize}
    \end{itemize}
\end{enumerate}

\subsection{Question Answering (QA)}
\textbf{Implementation of QA Models}
    \begin{itemize}
        \item \textbf{Llama 3.1 (\texttt{llama3.1:8b}):}
        \begin{itemize}
            \item Implemented using the Ollama API.
            \item Question and related context were provided directly to the model.
            \item The model generated detailed answers based on the context and its pre-trained knowledge.
        \end{itemize}
        \item \textbf{\texttt{deepset/roberta-base-squad2}:}
        \begin{itemize}
            \item Implemented using Hugging Face's Transformers library.
            \item The model was pre-trained on the SQuAD2.0 dataset and fine-tuned on BioASQ questions.
            \item It provided concise, span-based answers, drawing from a static subset of the PubMed data rather than dynamically retrieved documents.
        \end{itemize}
    \end{itemize}

\section{Evaluation}
The evaluation of our BioASQ Task-13b system focused on independently assessing the performance of each model in the document retrieval and question-answering phases. Below, we outline the evaluation criteria and results for each model.

\subsection{Relevant Document Retrieval}
\begin{itemize}
    \item \textbf{Vector Space Models (VSM):}
    \begin{itemize}
        \item \textbf{Evaluation Metric:}\\
        The TF-IDF models (with raw counts and logarithmic smoothing) were evaluated based on cosine similarity, ranking documents by their relevance scores for a given query.
        \item \textbf{Results:}\\
        The logarithmic smoothing model showed improved balance in rankings for queries involving common biomedical terms compared to the raw count model.
    \end{itemize}
    \item \textbf{BM25:}
    \begin{itemize}
        \item \textbf{Evaluation Metric:}\\
        The BM25 model ranked documents based on a probability score, reflecting the likelihood of document relevance to a given query.
        \item \textbf{Results:}\\
        BM25 demonstrated robust performance, especially for queries where document length normalization played a critical role. Its parameter tuning improved retrieval effectiveness across varied query types.
    \end{itemize}
\end{itemize}

\subsection{Question Answering (QA)}
\begin{enumerate}
    \item \textbf{Llama 3.1 (\texttt{llama3.1:8b}):}
    \begin{itemize}
        \item \textbf{Evaluation Metric:}\\
        The outputs of the Llama 3.1 model were evaluated manually for correctness and relevance.
        \item \textbf{Results:}
        \begin{itemize}
            \item The model provided detailed and contextually appropriate answers for $64$ out of $85$ queries.
            \item Some answers included irrelevant information, reflecting the model’s reliance on its pre-trained knowledge rather than document-specific inputs.
        \end{itemize}
    \end{itemize}
    \item \textbf{\texttt{deepset/roberta-base-squad2}:}
    \begin{itemize}
        \item \textbf{Evaluation Metrics:}
        \begin{itemize}
            \item \textbf{Exact Match (EM):} Measures the percentage of answers that exactly match the ground truth.
            \item \textbf{F1 Score:} Evaluates the overlap between the predicted and ground truth answers by considering precision and recall.
        \end{itemize}
        \item \textbf{Results:}
        \begin{itemize}
            \item \textbf{Average Exact Match (EM):} $0.817$
            \item \textbf{Average F1 Score:} $0.856$
        \end{itemize}
        These results, achieved on the validation dataset, indicate that the model effectively identified correct answer spans for most queries, balancing precision and recall well.
    \end{itemize}
\end{enumerate}

\subsection{Summary of Evaluation}
\begin{itemize}
    \item Retrieval models effectively ranked relevant documents, with BM25 outperforming the TF-IDF variants in scenarios involving lengthy or complex queries.
    \item Llama 3.1 demonstrated versatility in generating detailed answers, though it required manual evaluation due to the lack of input from retrieved documents.
    \item \texttt{deepset/roberta-base-squad2} excelled in span-based QA tasks, achieving high accuracy and consistency as reflected in the EM and F1 scores.
\end{itemize}
The independent evaluation of these models provided valuable insights into their performance and highlighted areas for future integration and improvement in biomedical question-answering systems.

\section{Challenges}
The development and evaluation of our system for BioASQ Task-13b presented several challenges, stemming from the complexity of the task, the size of the dataset, and the computational demands of advanced NLP models. Below, we outline the key challenges encountered:

\begin{enumerate}
    \item \textbf{Dataset Size and Preprocessing}
    \begin{itemize}
        \item The PubMed dataset, exceeding $100GB$, posed significant challenges in terms of storage, management, and processing.
        \item Efficiently indexing and preprocessing such a large corpus required substantial memory and computational resources. Standard text normalization tasks such as tokenization, stopword removal, and vectorization became time-intensive due to the dataset's scale.
    \end{itemize}
    \item \textbf{Computational Complexity for Fine-Tuning}
    \begin{itemize}
        \item Fine-tuning large language models such as Llama 3.1 or domain-specific models like \texttt{deepset/roberta-base-squad2} is computationally expensive.
        \item Limited access to high-performance GPUs constrained the fine-tuning process, impacting the ability to fully adapt models to the biomedical domain.
    \end{itemize}
    \item \textbf{Time and Resource Constraints}
    \begin{itemize}
        \item The iterative process of model training, evaluation, and debugging was time-consuming, further compounded by limited computational resources.
        \item The size and complexity of the task required significant time for data preparation, model execution, and result analysis.
    \end{itemize}
\end{enumerate}
Addressing these challenges would require greater computational resources, improved dataset-handling strategies, and potential workflow integration. Despite these obstacles, our modular approach allowed for a comprehensive evaluation of each model’s capabilities.

\section{Related Works}
Biomedical question answering has been an active area of research, with prior works employing various approaches, including traditional information retrieval methods, fine-tuned machine learning models, and large language models. Below, we compare some notable solutions to BioASQ tasks and their reported performances.


\begin{table}[]
\scriptsize
    \begin{tabular}{|p{2cm}|p{4cm}|p{2cm}|p{3cm}|}
        \hline
        Model/Approach & Document Retrieval Metric & QA Metric & Results \\
        \hline
        \hline
        PubMedBERT + BM25 \cite{gu_domain-specific_2021} & Mean Average Precision (MAP) & F1 Score & MAP: 0.404, F1: 0.753\\
        \hline
        BioBERT + BM25 \cite{lee_biobert_2019} & Mean Reciprocal Rank (MRR) & Exact Match (EM), F1 & MRR: 0.416, EM: 0.692, F1: 0.771\\
        \hline
        Llama 2 (7B) \cite{tran_bioinstruct_2024} & Manual (QA relevance scoring)  & Manual & High relevance for general biomedical questions\\
        \hline
        BioASQ Official Baseline \cite{krithara_bioasq_2024} & Mean Average Precision (MAP) & F1 Score & MAP: 0.308, F1: 0.622\\
        \hline
        RoBERTa + BM25 \cite{noauthor_somosnlp-hackathon-2022roberta-base-biomedical-es-squad2-es_2024} & Normalized Discounted Cumulative Gain (NDCG) & Exact Match (EM), F1 & NDCG: 0.372, EM: 0.716, F1: 0.782\\
        \hline
    \end{tabular}
    \caption{Top 5 results from related papers}
    \label{tab:my_label}
\end{table}

\subsection{Comparison and Observations}
\begin{enumerate}
    \item \textbf{Traditional Retrieval Models with BERT Variants:}
    \begin{itemize}
        \item PubMedBERT and BioBERT models have shown notable improvements in biomedical tasks, particularly when combined with BM25 for document retrieval.
        \item BioBERT, fine-tuned on biomedical literature, demonstrates higher EM and F1 scores compared to the official baseline.
    \end{itemize}
    \item \textbf{Large Language Models (LLMs):}
    \begin{itemize}
        \item Models like Llama 2, while evaluated manually, excel in generating contextually relevant answers, highlighting their potential in open-domain biomedical QA.
    \end{itemize}
    \item \textbf{Baseline Systems:}
    \begin{itemize}
        \item The official BioASQ baselines provide a reference point for evaluating newer models. Our \texttt{roberta-base-squad2} outperforms the baseline in both EM and F1 metrics, showcasing the effectiveness of fine-tuned span-based QA models.
    \end{itemize}
    \item \textbf{RoBERTa Variants:}
    \begin{itemize}
        \item RoBERTa, particularly when paired with retrieval models like BM25, demonstrates competitive performance, striking a balance between retrieval and QA tasks.
    \end{itemize}
\end{enumerate}
Our project extends this body of work by independently evaluating multiple retrieval and QA models on the BioASQ Task-13b dataset. The insights gained can inform future efforts in designing integrated workflows and optimizing model performance.

\section{Conclusion}
In this project, we independently evaluated multiple models for document retrieval and question answering in the context of BioASQ Task-13b. Retrieval models like TF-IDF and BM25 effectively ranked documents based on relevance, while QA models such as Llama 3.1 and \texttt{deepset/roberta-base-squad2} demonstrated strong performance in generating accurate answers, with the latter achieving high Exact Match and F1 scores. Although the components were not integrated into a unified workflow, this modular approach provided valuable insights into the individual capabilities of each model. These findings highlight the potential for future systems to combine these models for enhanced performance in biomedical question answering.

\bibliography{references}
\end{document}

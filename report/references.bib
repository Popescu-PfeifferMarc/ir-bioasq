
@misc{gu_domain-specific_2021,
	title = {Domain-{Specific} {Language} {Model} {Pretraining} for {Biomedical} {Natural} {Language} {Processing}},
	url = {http://arxiv.org/abs/2007.15779},
	doi = {10.48550/arXiv.2007.15779},
	abstract = {Pretraining large neural language models, such as BERT, has led to impressive gains on many natural language processing (NLP) tasks. However, most pretraining efforts focus on general domain corpora, such as newswire and Web. A prevailing assumption is that even domain-specific pretraining can benefit by starting from general-domain language models. In this paper, we challenge this assumption by showing that for domains with abundant unlabeled text, such as biomedicine, pretraining language models from scratch results in substantial gains over continual pretraining of general-domain language models. To facilitate this investigation, we compile a comprehensive biomedical NLP benchmark from publicly-available datasets. Our experiments show that domain-specific pretraining serves as a solid foundation for a wide range of biomedical NLP tasks, leading to new state-of-the-art results across the board. Further, in conducting a thorough evaluation of modeling choices, both for pretraining and task-specific fine-tuning, we discover that some common practices are unnecessary with BERT models, such as using complex tagging schemes in named entity recognition (NER). To help accelerate research in biomedical NLP, we have released our state-of-the-art pretrained and task-specific models for the community, and created a leaderboard featuring our BLURB benchmark (short for Biomedical Language Understanding \& Reasoning Benchmark) at https://aka.ms/BLURB.},
	urldate = {2024-11-27},
	publisher = {arXiv},
	author = {Gu, Yu and Tinn, Robert and Cheng, Hao and Lucas, Michael and Usuyama, Naoto and Liu, Xiaodong and Naumann, Tristan and Gao, Jianfeng and Poon, Hoifung},
	month = sep,
	year = {2021},
	note = {arXiv:2007.15779},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/abd/Zotero/storage/5KLFSU5M/Gu et al. - 2021 - Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing.pdf:application/pdf;Snapshot:/Users/abd/Zotero/storage/3BERW7DF/2007.html:text/html},
}

@article{krithara_bioasq_2024,
	title = {{BioASQ} {Synergy}: a dialogue between question-answering systems and biomedical experts for promoting {COVID}-19 research},
	volume = {31},
	issn = {1527-974X},
	shorttitle = {{BioASQ} {Synergy}},
	url = {https://doi.org/10.1093/jamia/ocae232},
	doi = {10.1093/jamia/ocae232},
	abstract = {This article presents the novel BioASQ Synergy research process which aims to facilitate the interaction between biomedical experts and automated question-answering systems.The proposed research allows systems to provide answers to emerging questions, which in turn are assessed by experts. The assessment of the experts is fed back to the systems, together with new questions. With this iteration, we aim to facilitate the incremental understanding of a developing problem and contribute to solution discovery.The results suggest that the proposed approach can assist researchers to navigate available resources. The experts seem to be very satisfied with the quality of the ideal answers provided by the systems, suggesting that such systems are already useful in answering open research questions.BioASQ Synergy aspires to provide a tool that gives the experts easy and personalized access to the latest findings in a fast-growing corpus of material.In this article, we envisioned BioASQ Synergy as a continuous dialogue between experts and systems to issue open questions. We ran an initial proof-of-concept of the approach, in order to evaluate its usefulness, both from the side of the experts, as well as from the side of the participating systems.},
	number = {11},
	urldate = {2024-11-27},
	journal = {Journal of the American Medical Informatics Association},
	author = {Krithara, Anastasia and Nentidis, Anastasios and Vandorou, Eirini and Katsimpras, Georgios and Almirantis, Yannis and Arnal, Magda and Bunevicius, Adomas and Farre-Maduell, Eulalia and Kassiss, Maya and Konstantakos, Vasileios and Matis-Mitchell, Sherri and Polychronopoulos, Dimitris and Rodriguez-Pascual, Jesus and Samaras, Eleftherios G and Samiotaki, Martina and Sanoudou, Despina and Vozi, Aspasia and Paliouras, Georgios},
	month = nov,
	year = {2024},
	pages = {2689--2698},
	file = {Full Text PDF:/Users/abd/Zotero/storage/Y7HWB6FZ/Krithara et al. - 2024 - BioASQ Synergy a dialogue between question-answering systems and biomedical experts for promoting C.pdf:application/pdf;Snapshot:/Users/abd/Zotero/storage/UZI5WV96/7740553.html:text/html},
}

@misc{tran_bioinstruct_2024,
	title = {{BioInstruct}: {Instruction} {Tuning} of {Large} {Language} {Models} for {Biomedical} {Natural} {Language} {Processing}},
	shorttitle = {{BioInstruct}},
	url = {http://arxiv.org/abs/2310.19975},
	doi = {10.48550/arXiv.2310.19975},
	abstract = {Objective: To enhance the performance of large language models (LLMs) in biomedical natural language processing (BioNLP) by introducing a domain-specific instruction dataset and examining its impact when combined with multitask learning principles. Materials and Methods: We created the BioInstruct, comprising 25,005 instructions to instruction-tune LLMs(LLaMA 1 \& 2, 7B \& 13B version). The instructions were created by prompting the GPT-4 language model with three-seed samples randomly drawn from an 80 human curated instructions. We employed Low-Rank Adaptation(LoRA) for parameter-efficient fine-tuning. We then evaluated these instruction-tuned LLMs on several BioNLP tasks, which can be grouped into three major categories: question answering(QA), information extraction(IE), and text generation(GEN). We also examined whether categories(e.g., QA, IE, and generation) of instructions impact model performance.
Results and Discussion: Comparing with LLMs without instruction-tuned, our instruction-tuned LLMs demonstrated marked performance gains: 17.3\% in QA on average accuracy metric, 5.7\% in IE on average F1 metric, and 96\% in Generation tasks on average GPT4 score metric. Our 7B-parameter instruction-tuned LLaMA 1 model was competitive or even surpassed other LLMs in the biomedical domain that were also fine-tuned from LLaMA 1 with vast domainspecific data or a variety of tasks. Our results also show that the performance gain is significantly higher when instruction fine-tuning is conducted with closely related tasks. Our findings align with the observations of multi-task learning, suggesting the synergies between two tasks.
Conclusion: The BioInstruct dataset serves as a valuable resource and instruction tuned LLMs lead to the best performing BioNLP applications.},
	language = {en},
	urldate = {2024-11-27},
	publisher = {arXiv},
	author = {Tran, Hieu and Yang, Zhichao and Yao, Zonghai and Yu, Hong},
	month = jun,
	year = {2024},
	note = {arXiv:2310.19975 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {PDF:/Users/abd/Zotero/storage/ZU72HUYF/Tran et al. - 2024 - BioInstruct Instruction Tuning of Large Language Models for Biomedical Natural Language Processing.pdf:application/pdf},
}

@misc{noauthor_somosnlp-hackathon-2022roberta-base-biomedical-es-squad2-es_2024,
	title = {somosnlp-hackathon-2022/roberta-base-biomedical-es-squad2-es {\textperiodcentered} {Hugging} {Face}},
	url = {https://huggingface.co/somosnlp-hackathon-2022/roberta-base-biomedical-es-squad2-es},
	abstract = {We{\textquoteright}re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2024-11-27},
	month = jan,
	year = {2024},
	file = {Snapshot:/Users/abd/Zotero/storage/W7WYLJ4J/roberta-base-biomedical-es-squad2-es.html:text/html},
}

@misc{lee_biobert_2019,
	title = {{BioBERT}: a pre-trained biomedical language representation model for biomedical text mining},
	shorttitle = {{BioBERT}},
	url = {http://arxiv.org/abs/1901.08746},
	doi = {10.48550/arXiv.1901.08746},
	abstract = {Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows. With the progress in natural language processing (NLP), extracting valuable information from biomedical literature has gained popularity among researchers, and deep learning has boosted the development of effective biomedical text mining models. However, directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora. In this article, we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora. We introduce BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining), which is a domain-specific language representation model pre-trained on large-scale biomedical corpora. With almost the same architecture across tasks, BioBERT largely outperforms BERT and previous state-of-the-art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora. While BERT obtains performance comparable to that of previous state-of-the-art models, BioBERT significantly outperforms them on the following three representative biomedical text mining tasks: biomedical named entity recognition (0.62\% F1 score improvement), biomedical relation extraction (2.80\% F1 score improvement) and biomedical question answering (12.24\% MRR improvement). Our analysis results show that pre-training BERT on biomedical corpora helps it to understand complex biomedical texts. We make the pre-trained weights of BioBERT freely available at https://github.com/naver/biobert-pretrained, and the source code for fine-tuning BioBERT available at https://github.com/dmis-lab/biobert.},
	urldate = {2024-11-27},
	publisher = {arXiv},
	author = {Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},
	month = oct,
	year = {2019},
	note = {arXiv:1901.08746},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/abd/Zotero/storage/5LPD65PJ/Lee et al. - 2019 - BioBERT a pre-trained biomedical language representation model for biomedical text mining.pdf:application/pdf;Snapshot:/Users/abd/Zotero/storage/3N5I7MXX/1901.html:text/html},
}
